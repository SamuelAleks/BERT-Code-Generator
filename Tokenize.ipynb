{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -p /home/admin/projects/active/Bert Agent/.conda ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your dataset is stored in a file named \"dataset.txt\"\n",
    "data_file = \"concatenate.txt\"\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to preprocess each line of the dataset\n",
    "def preprocess_line(line):\n",
    "    # Split the line based on the delimiter\n",
    "    parts = line.strip().split(\" +++$+++ \")\n",
    "    # Extract the primary content (last part)\n",
    "    content = parts[-1]\n",
    "    return content\n",
    "\n",
    "# Function to tokenize and preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Add [CLS] and [SEP] tokens\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    # Convert tokens to ids\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # Convert to tensor\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    return input_ids\n",
    "\n",
    "# Read and preprocess the dataset\n",
    "input_sequences = []\n",
    "with open(data_file, 'r') as f:\n",
    "    for line in f:\n",
    "        content = preprocess_line(line)\n",
    "        input_ids = preprocess_text(content)\n",
    "        input_sequences.append(input_ids)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = [torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in input_sequences]\n",
    "\n",
    "# Convert to tensor dataset\n",
    "input_sequences = torch.stack(input_sequences)\n",
    "labels = torch.zeros(input_sequences.shape[0], dtype=torch.long)  # Dummy labels for example\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(input_sequences, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Fine-tune BERT\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):  # Adjust number of epochs as needed\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs)[0]\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
